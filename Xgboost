Xgboost全程是eXtreme Gradient Boosting。
最大的优点就是能够自动利用CPU的多线程进行并行，同时在算法上加以改进提高精度。
（是Boosting Tree的一种实现，它对损失函数进行了二阶泰勒展开，同时利用到一阶和二阶导数，并引入了适用于树模型的正则项控制模型的复杂度。
Xgboost的正则项包括树的叶子节点个数和每个叶子节点输出分数的L2平方和）。

大致有三个优点：高效、准确度、模型的交互性
1、高效
xgboost借助OpenMP，能自动利用单机CPU的多核进行并行计算
Mac上的Clang对OpenMP的支持较差，所以默认情况下只能单核运行
xgboost自定义了一个数据矩阵类DMatrix，会在训练开始时进行一遍预处理，从而提高之后每次迭代的效率
 它类似于梯度上升框架，但是更加高效。它兼具线性模型求解器和树学习算法。因此，它快速的秘诀在于算法在单机上也可以并行计算的能力。
 这使得xgboost至少比现有的梯度上升实现有至少10倍的提升。它提供多种目标函数，包括回归，分类和排序。
2、准确性
准确度提升的主要原因在于，xgboost的模型和传统的GBDT相比加入了对于模型复杂度的控制以及后期的剪枝处理，使得学习出来的模型更加不容易过拟合。
由于它在预测性能上的强大但是相对缓慢的实现，"xgboost" 成为很多比赛的理想选择。它还有做交叉验证和发现关键变量的额外功能。
在优化模型时，这个算法还有非常多的参数需要调整。


3、模型的交互性

能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数
允许用户在交叉验证时自定义误差衡量方法，例如回归中使用RMSE还是RMSLE，分类中使用AUC，分类错误率或是F1-score。甚至是在希格斯子比赛中的“奇葩”衡量标准AMS
交叉验证时可以返回模型在每一折作为预测集时的预测结果，方便构建ensemble模型。
允许用户先迭代1000次，查看此时模型的预测效果，然后继续迭代1000次，最后模型等价于一次性迭代2000次
可以知道每棵树将样本分类到哪片叶子上，facebook介绍过如何利用这个信息提高模型的表现
可以计算变量重要性并画出树状图
可以选择使用线性模型替代树模型，从而得到带L1+L2惩罚的线性回归或者logistic回归
